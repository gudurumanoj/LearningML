Techniques to improve smaller models
- Distillation (Knowledge distillation techniques)
- Quantization
- Training on large quality datasets 

### Cosmopedia v1 to v2
[[Pipeline Summary]] of cosmopedia v1
- Training models with content generated from larger models with same prompts have shown no improvement on benchmarks

#### Prompt
Topic, seed sample, style of generation (audience and type)
- Topic curation
	- unsupervised clustering in v1 limits our control over topics and the quality of web samples in each cluster if not filtered
	- For v2, 5000 topics belonging to 51 categories and with few subtopics,  asked Mixtral to generate subsubtopics under  for certain topics (There by getting predefined list of 34,000 topics)
	- A search tool is implemented to retrieve the most relevant pages for each topic from the FineWeb CC-MAIN-2024–10 and CC-MAIN-2023–50 dumps. ​​For each query 1000 pages are retrieved
- Generation style
	- Ablation studies to decide, each experiment twice with two different seeds and averaged the scores between the two runs.
	- Textbooks with the same web textbook prompts but targeted two different audiences: middle school students and college students are generated
### Fineweb-edu
- Trained an [educational classifier](https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier) using annotations generated by Llama3-70B-Instruct
- Used this classifier to retain only the most educational webpages
### Stack-edu Python
- Trained a [code classifier](https://huggingface.co/HuggingFaceTB/python-edu-scorer) using annotations generated by Llama3-70B-Instruct
- Applied this classifier to a Python subset of the StarCoder models training corpus (retained those with score of >=4)

### Training
Training parameters can be found [[Training/Smollm|here]] 
